{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary table of most frequently used neural network layers\n",
    "\n",
    "\n",
    "| Pytorch name  | Formula   | Trainable parameters  | Element wise | Used as last layer | Used as inner layer |\n",
    "|----|----|----|----|----|----|\n",
    "| `torch.nn.Linear(in_features=10, out_features=5)`  | $ f_i(x) = \\sum_j W_{i,j} x_j + b_i $ or $ f(x) = W x + b $ | `W.shape ==  (5, 10)`, `b.shape == (5,)`  | No | Yes, for regression. | Yes, alternated with non-linear activation function layers. |\n",
    "| `torch.nn.Embedding(embedding_dim=256, num_embeddings=10_000)`  | $ f_i(j) = E_{i,j} $ | `E.shape ==  (256, 10_000)`  | No | No, only as input layer for integer identifiers. | Very rarely. |\n",
    "| `torch.nn.Conv1D(in_channels=16, out_channels=32, kernel_size=5)` | See slides | `W.shape ==  (32, 16, 5)`, `b.shape == (32,)` |  No | Rarely | Yes, for sequence transformation. |\n",
    "| `torch.nn.Conv2D(in_channels=16, out_channels=32, kernel_size=(3, 3))` | See slides | `W.shape ==  (32, 16, 3, 3)`, `b.shape == (32,)` |  No | Rarely | Yes, for image transformation. |\n",
    "| `torch.nn.ReLU()`| $ f_i(x) = max(0, x_i) $ | None | Yes | No | Yes, as a non-linear activation function between parametrized layers. |\n",
    "| `torch.nn.Sigmoid()` | $ f_i(x) = \\frac{1}{1 + e^{-x_i}} $ | None | Yes | Yes, for binary classifiers. | Not in modern architectures. |\n",
    "| `torch.nn.Softmax()` | $ f_i(x) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $ | None | No | Yes, for multiclass classifiers. | Sometimes, e.g. for attention mechanisms in transformers. |\n",
    "| `torch.nn.Dropout(p=0.2)`| $ f_i(x) = 0 $ with probability $p$ or $ f_i(x) = x_i $ otherwise | None | Yes | No | Yes, mostly to prevent overfitting while training. |\n",
    "| `torch.nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))`| See slides. | None | No | No | Yes, mostly to reduce spatial dimensionality in vision networks. |\n",
    "| `torch.nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))`| See slides. | None | No | No | Yes, mostly to reduce spatial dimensionality in vision networks. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2350,  0.7306, -0.3485,  0.2325, -0.3516]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = torch.nn.Linear(in_features=10, out_features=5)\n",
    "input_data = torch.randn(1, 10)\n",
    "output_data = linear(input_data)\n",
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Linear                                   [1, 5]                    55\n",
       "==========================================================================================\n",
       "Total params: 55\n",
       "Trainable params: 55\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(linear, input_size=input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7314,  0.1365,  0.8833,  ..., -0.4714,  0.2999, -0.8848],\n",
       "        [-1.4625,  1.5284, -0.1809,  ..., -0.2909,  0.8182, -1.4778],\n",
       "        [ 0.0916, -1.8457,  0.2169,  ..., -0.7263,  0.7409, -1.1889],\n",
       "        ...,\n",
       "        [-0.6754,  0.8027, -1.0114,  ...,  0.4772,  0.9615, -0.2119],\n",
       "        [ 1.5572, -1.0840,  1.3718,  ...,  0.4971,  0.5859, -2.3298],\n",
       "        [-0.2463, -1.7743,  1.2568,  ..., -1.0989, -1.1573,  1.3765]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = torch.nn.Embedding(embedding_dim=256, num_embeddings=10_000)\n",
    "input_data = torch.randint(low=0, high=10_000, size=(10,))\n",
    "output_data = embedding(input_data)\n",
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torchinfo.summary does not work with Embedding layers?\n",
    "output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4152,  0.3532,  0.5414,  ..., -0.1657,  0.4355, -0.5072],\n",
       "         [-0.2555, -0.0250,  0.0893,  ..., -0.2941, -0.2616,  0.1319],\n",
       "         [ 0.2198, -0.2883, -0.0334,  ..., -0.4723,  0.2367,  0.2849],\n",
       "         ...,\n",
       "         [ 0.5063,  0.7722,  0.8691,  ...,  0.5755, -0.5086,  0.1494],\n",
       "         [-0.1414,  0.1739,  0.0394,  ..., -0.7839, -0.7505, -0.2803],\n",
       "         [ 0.0523, -0.5452,  0.2403,  ..., -0.1316,  0.9275, -0.7144]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d = torch.nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=\"same\")\n",
    "input_data = torch.randn(1, 16, 100)  # (batch_size, in_channels, input_length)\n",
    "output_data = conv1d(input_data)\n",
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 16, 5]), torch.Size([32]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d.weight.shape, conv1d.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Conv1d                                   [1, 32, 100]              2,592\n",
       "==========================================================================================\n",
       "Total params: 2,592\n",
       "Trainable params: 2,592\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.26\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.03\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.04\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(conv1d, input_size=input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1282,  0.3398, -0.0484,  ...,  0.1026,  0.1006,  0.3596],\n",
       "          [-0.5153, -0.1646, -0.1448,  ..., -0.1040, -0.4692,  0.3613],\n",
       "          [ 0.2917,  0.3874, -0.8717,  ..., -0.6438,  0.4476, -0.4428],\n",
       "          ...,\n",
       "          [ 0.5198,  0.9007, -0.1777,  ...,  0.5318, -1.5554,  0.2853],\n",
       "          [ 0.3252,  0.4171, -0.0967,  ...,  0.2620, -0.4453, -0.4608],\n",
       "          [ 0.3799,  0.2737,  0.1409,  ..., -0.0295, -0.1304, -0.2311]],\n",
       "\n",
       "         [[-0.0871,  0.0276, -0.4660,  ..., -0.8063,  0.3991,  0.7173],\n",
       "          [ 0.6543,  0.6663,  0.3561,  ..., -0.3361,  0.4271,  0.9505],\n",
       "          [ 0.2346,  0.3326,  0.1194,  ...,  0.4064, -0.2679, -0.7752],\n",
       "          ...,\n",
       "          [-0.2633,  0.1063,  0.0273,  ...,  0.6872, -0.3092, -0.0640],\n",
       "          [ 0.0101,  0.0037, -0.1582,  ..., -0.5729, -0.4625,  0.1141],\n",
       "          [ 0.1168, -0.0920,  0.4790,  ..., -0.3043, -0.2561,  0.3524]],\n",
       "\n",
       "         [[ 0.4074, -0.3504,  0.2201,  ...,  0.3430,  0.5963, -0.1674],\n",
       "          [ 0.5389, -0.2693,  0.4214,  ...,  0.2123, -0.2849,  0.2955],\n",
       "          [ 0.2572, -0.6035,  1.5576,  ...,  0.0775,  0.2789,  0.9843],\n",
       "          ...,\n",
       "          [ 0.1520,  0.3927, -0.4305,  ..., -0.0869, -0.3570, -0.2327],\n",
       "          [ 0.3113,  0.2171,  0.9634,  ...,  0.8984, -0.0148,  0.0503],\n",
       "          [-0.1630, -0.1782, -0.0030,  ...,  0.5071,  0.1990,  0.0545]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.4673,  0.0369,  0.1368,  ..., -0.0564,  0.0428, -0.0790],\n",
       "          [-0.1469, -0.1520,  0.0683,  ...,  0.1618, -0.4637, -0.0502],\n",
       "          [ 0.5822,  0.4894, -0.2691,  ..., -0.1409,  0.2189,  0.2094],\n",
       "          ...,\n",
       "          [-0.4417,  1.2704,  0.8655,  ...,  1.0253, -0.2563,  0.9408],\n",
       "          [ 0.6465, -0.2769, -0.8501,  ...,  0.0667, -0.2226, -0.2432],\n",
       "          [ 0.0754,  0.0351,  0.5926,  ...,  0.1855, -0.3522, -0.1510]],\n",
       "\n",
       "         [[-0.4229, -0.5272, -0.3557,  ..., -0.3258, -0.9732, -0.0141],\n",
       "          [-0.2301, -0.2966, -0.1656,  ...,  0.3794, -0.5888,  0.5045],\n",
       "          [-0.2667,  0.1808, -0.3806,  ..., -0.4624, -0.9555,  0.2161],\n",
       "          ...,\n",
       "          [ 0.1367, -0.7768,  0.1079,  ..., -1.1703, -0.0972, -0.5116],\n",
       "          [ 0.2231, -0.2017,  0.5891,  ..., -0.1620, -0.4013, -0.7499],\n",
       "          [ 0.4453, -0.5237,  0.1868,  ...,  0.1408, -0.4868, -0.0801]],\n",
       "\n",
       "         [[-0.0144,  0.3831, -0.4923,  ..., -0.0331, -0.3744,  0.0632],\n",
       "          [-0.2767,  0.0781, -0.2408,  ...,  0.1304,  0.7452, -0.4060],\n",
       "          [ 0.2991, -0.2821,  0.8153,  ...,  0.3830,  0.0193,  0.2504],\n",
       "          ...,\n",
       "          [ 0.4644,  0.5213,  0.7746,  ..., -0.4134, -0.3605, -0.0586],\n",
       "          [-0.4181, -0.4293, -0.2235,  ..., -0.3431, -0.9142, -0.4615],\n",
       "          [-0.3547, -0.9672, -0.5808,  ..., -0.2290, -0.0935,  0.2058]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=\"same\")\n",
    "input_data = torch.randn(1, 16, 100, 100)  # (batch_size, in_channels, input_height, input_width)\n",
    "output_data = conv2d(input_data)\n",
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Conv2d                                   [1, 32, 100, 100]         12,832\n",
       "==========================================================================================\n",
       "Total params: 12,832\n",
       "Trainable params: 12,832\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 128.32\n",
       "==========================================================================================\n",
       "Input size (MB): 0.64\n",
       "Forward/backward pass size (MB): 2.56\n",
       "Params size (MB): 0.05\n",
       "Estimated Total Size (MB): 3.25\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(conv2d, input_size=input_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
